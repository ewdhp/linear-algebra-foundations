# Normed Spaces

## üìñ What You'll Learn

Normed spaces provide a rigorous mathematical framework for measuring the "size" or "length" of vectors in any vector space. Understanding norms is essential for optimization, regularization, and measuring distances in machine learning applications.

## üìÇ Subtopics

This section contains the following specialized normed spaces:

1. **[Hilbert Spaces](./hilbert-spaces/)** - Complete inner product spaces with perfect geometry
   - **[Kernel Methods & RKHS](./hilbert-spaces/kernel-methods-rkhs/)** - SVMs, Gaussian Processes, and Neural Tangent Kernels
2. **[Banach Spaces](./banach-spaces/)** - Complete normed spaces for optimization and neural operators

### Core Concepts

1. **Vector Norms**
   - Definition: A function ‚Äñ¬∑‚Äñ : V ‚Üí ‚Ñù satisfying three axioms
   - Non-negativity: ‚Äñx‚Äñ ‚â• 0, with ‚Äñx‚Äñ = 0 iff x = 0
   - Homogeneity: ‚ÄñŒ±x‚Äñ = |Œ±|‚Äñx‚Äñ for any scalar Œ±
   - Triangle inequality: ‚Äñx + y‚Äñ ‚â§ ‚Äñx‚Äñ + ‚Äñy‚Äñ

2. **Common Norms (Lp Norms)**
   - **L1 Norm (Manhattan)**: ‚Äñx‚Äñ‚ÇÅ = Œ£|x·µ¢|
   - **L2 Norm (Euclidean)**: ‚Äñx‚Äñ‚ÇÇ = ‚àö(Œ£x·µ¢¬≤)
   - **L‚àû Norm (Maximum)**: ‚Äñx‚Äñ‚àû = max|x·µ¢|
   - **General Lp Norm**: ‚Äñx‚Äñ‚Çö = (Œ£|x·µ¢|·µñ)^(1/p)

3. **Norm Equivalence**
   - All norms in finite-dimensional spaces are equivalent
   - Relationships between different norms
   - Choosing norms for specific applications

4. **Matrix Norms**
   - Induced norms: ‚ÄñA‚Äñ = sup{‚ÄñAx‚Äñ / ‚Äñx‚Äñ : x ‚â† 0}
   - Frobenius norm: ‚ÄñA‚Äñ_F = ‚àö(Œ£Œ£a·µ¢‚±º¬≤)
   - Spectral norm: ‚ÄñA‚Äñ‚ÇÇ = largest singular value
   - Nuclear norm: sum of singular values

5. **Unit Balls**
   - Geometric interpretation of different norms
   - Unit ball in L1: diamond/cross-polytope
   - Unit ball in L2: sphere
   - Unit ball in L‚àû: hypercube

6. **Normed Vector Spaces**
   - Structure: (V, ‚Äñ¬∑‚Äñ)
   - Metric induced by norm: d(x,y) = ‚Äñx - y‚Äñ
   - Convergence and continuity in normed spaces
   - Cauchy sequences and completeness

7. **Dual Norms**
   - Definition: ‚Äñz‚Äñ* = sup{‚ü®x,z‚ü© : ‚Äñx‚Äñ ‚â§ 1}
   - Relationship: (Lp)* = Lq where 1/p + 1/q = 1
   - Applications in optimization duality

## ü§ñ Machine Learning Applications

### Regularization
- **L1 Regularization (Lasso)**:
  - Penalty term: Œª‚Äñw‚Äñ‚ÇÅ = ŒªŒ£|w·µ¢|
  - Promotes sparsity (many weights become exactly zero)
  - Feature selection built into optimization
  - Applications: compressed sensing, sparse models
  
- **L2 Regularization (Ridge)**:
  - Penalty term: Œª‚Äñw‚Äñ‚ÇÇ¬≤ = ŒªŒ£w·µ¢¬≤
  - Shrinks all weights proportionally
  - Prevents overfitting by limiting model complexity
  - Applications: neural networks, linear regression
  
- **Elastic Net**: Combines L1 and L2: Œ±‚Äñw‚Äñ‚ÇÅ + Œ≤‚Äñw‚Äñ‚ÇÇ¬≤
  
- **Group Lasso**: Œ£Œ£‚Äñw_g‚Äñ‚ÇÇ for structured sparsity

### Loss Functions
- **Mean Absolute Error (L1 Loss)**: ‚Äñy - ≈∑‚Äñ‚ÇÅ
  - Robust to outliers
  - Used in robust regression
  
- **Mean Squared Error (L2 Loss)**: ‚Äñy - ≈∑‚Äñ‚ÇÇ¬≤
  - Penalizes large errors more heavily
  - Smooth gradients for optimization
  - Standard in regression problems
  
- **Huber Loss**: Combines L1 and L2 properties
  
- **Hinge Loss**: max(0, 1 - yf(x)) for SVM

### Distance Metrics & Similarity
- **Cosine Similarity**: Related to L2 norm
- **Manhattan Distance**: L1 distance for grid-like spaces
- **Euclidean Distance**: L2 distance standard metric
- **Chebyshev Distance**: L‚àû distance
- **k-NN Classification**: Choice of norm affects neighborhoods

### Neural Network Training
- **Weight Decay**: L2 penalty on network weights
- **Dropout**: Implicitly enforces norm constraints
- **Batch Normalization**: Normalizing activations
- **Gradient Clipping**: Constraining gradient norms
- **Lipschitz Constraints**: Bounding operator norms for stability

### Optimization
- **Gradient Descent**: Direction opposite to gradient norm
- **Proximal Methods**: Utilizing specific norm structures
- **Projected Gradient**: Constraining to norm balls
- **ADMM**: Exploiting separable norm penalties

### Model Compression
- **Pruning**: Removing small-norm weights
- **Quantization**: Reducing precision while controlling norm
- **Low-rank Approximation**: Nuclear norm minimization
- **Knowledge Distillation**: Matching output distributions

## üìä Topics Covered

### Theoretical Foundations
- Axiomatic definition of norms
- Norm equivalence theorems
- Completion of normed spaces
- H√∂lder and Minkowski inequalities
- Banach space theory basics

### Computational Methods
- Computing various Lp norms
- Efficient matrix norm computation
- Proximal operators for common norms
- Subgradients for non-smooth norms
- Projection onto norm balls

### Advanced Concepts
- Dual norms and conjugate functions
- Quotient norms
- Operator norms
- Nuclear and trace norms
- Schatten norms

### Geometric Intuition
- Visualizing unit balls in different norms
- Sparsity patterns from L1 geometry
- Smoothness from L2 geometry
- Contour plots and level sets

## üíª What's Included

- **Comprehensive Theory**: Mathematical foundations and properties
- **Visual Intuition**: 2D/3D visualizations of different norm balls
- **Python Implementations**: NumPy and SciPy code for:
  - Computing various Lp norms
  - Implementing regularized regression
  - Visualizing norm geometries
  - Proximal operators for optimization
  - Matrix norms and spectral properties
- **Interactive Examples**: Comparing different norms and regularization
- **ML Applications**: Real-world examples with datasets
- **Exercises**: From basic norm computations to advanced regularization
- **Code Patterns**: Common ML implementations using norms

## üéØ Learning Outcomes

By the end of this section, you will be able to:

‚úÖ Define and compute various vector and matrix norms  
‚úÖ Understand geometric differences between L1, L2, and L‚àû norms  
‚úÖ Apply L1 regularization for sparse models (Lasso)  
‚úÖ Apply L2 regularization to prevent overfitting (Ridge)  
‚úÖ Choose appropriate loss functions for ML tasks  
‚úÖ Implement regularized optimization algorithms  
‚úÖ Understand why L1 promotes sparsity geometrically  
‚úÖ Use norms for distance metrics and similarity measures  
‚úÖ Apply gradient clipping and weight decay in neural networks  
‚úÖ Analyze model behavior through norm-based constraints  

## üìê Mathematical Summary

### Key Definitions

**Lp Norms:**
```
‚Äñx‚Äñ‚ÇÅ = Œ£|x·µ¢|                     (L1/Manhattan)
‚Äñx‚Äñ‚ÇÇ = ‚àö(Œ£x·µ¢¬≤)                   (L2/Euclidean)
‚Äñx‚Äñ‚Çö = (Œ£|x·µ¢|·µñ)^(1/p)            (General Lp)
‚Äñx‚Äñ‚àû = max|x·µ¢|                   (L‚àû/Maximum)
```

**Dual Norm Relationship:**
```
If 1/p + 1/q = 1, then (Lp)* = Lq

Examples:
(L1)* = L‚àû
(L2)* = L2 (self-dual)
```

**Matrix Norms:**
```
‚ÄñA‚Äñ‚ÇÇ = œÉ_max(A)                  (Spectral norm)
‚ÄñA‚Äñ_F = ‚àö(Œ£Œ£a·µ¢‚±º¬≤) = ‚àö(tr(A·µÄA))   (Frobenius)
‚ÄñA‚Äñ* = Œ£œÉ·µ¢                       (Nuclear norm)
```

**Regularized Loss:**
```
L(w) = Error(w) + ŒªR(w)

Ridge:  R(w) = ‚Äñw‚Äñ‚ÇÇ¬≤
Lasso:  R(w) = ‚Äñw‚Äñ‚ÇÅ
Elastic Net: R(w) = Œ±‚Äñw‚Äñ‚ÇÅ + Œ≤‚Äñw‚Äñ‚ÇÇ¬≤
```

## üî¨ Example Problems

### Problem 1: Computing Different Norms
Given vector x = [3, -4, 0, 2]:
- Compute ‚Äñx‚Äñ‚ÇÅ, ‚Äñx‚Äñ‚ÇÇ, ‚Äñx‚Äñ‚àû
- Sketch unit balls for each norm in 2D
- Explain geometric differences

### Problem 2: Lasso vs Ridge Regression
Given dataset with correlated features:
- Implement L1-regularized regression (Lasso)
- Implement L2-regularized regression (Ridge)
- Compare sparsity patterns
- Analyze feature selection behavior

### Problem 3: Optimal Regularization Parameter
For a regression problem:
- Use cross-validation to select Œª
- Plot validation error vs Œª
- Compare L1 and L2 regularization paths
- Visualize weight trajectories

### Problem 4: Gradient Clipping
In neural network training:
- Implement gradient norm clipping
- Compare training with/without clipping
- Analyze effect on convergence
- Prevent exploding gradients

## üìö Prerequisites

- Understanding of vectors and vector spaces
- Basic calculus and optimization
- Linear algebra fundamentals
- Familiarity with regression and loss functions

## üöÄ Next Steps

After mastering normed spaces, explore the specialized subtopics:

1. **[Hilbert Spaces](./hilbert-spaces/)** - Adding inner products to normed spaces
   - **[Kernel Methods & RKHS](./hilbert-spaces/kernel-methods-rkhs/)** - Applications to SVMs and GPs
2. **[Banach Spaces](./banach-spaces/)** - Complete normed spaces and functional analysis
3. **Optimization Theory** - Convex analysis and proximal methods
4. **Advanced Regularization** - Nuclear norms, structured sparsity
5. **Compressed Sensing** - L1 minimization for sparse recovery

## üéì Why This Matters for ML/AI

Understanding norms is crucial because:

- **Regularization prevents overfitting** - Essential for generalization
- **Different norms encode different prior beliefs** - L1 for sparsity, L2 for smoothness
- **Loss functions are norms** - Choosing the right metric for your problem
- **Optimization algorithms depend on geometry** - Norm structure affects convergence
- **Model compression uses norm constraints** - Pruning and quantization
- **Stability analysis requires operator norms** - Understanding model sensitivity
- **Distance metrics define neighborhoods** - Critical for k-NN, clustering

Norms are the foundation for understanding how to measure and constrain models!

## üìñ Recommended Reading

- *Functional Analysis* by Walter Rudin (Chapter 1)
- *Convex Optimization* by Boyd & Vandenberghe (Chapter 3)
- *Deep Learning* by Goodfellow et al. (Chapter 7.1)
- *The Elements of Statistical Learning* by Hastie et al. (Chapter 3.4)
- *High-Dimensional Statistics* by Wainwright (Chapter 2)

## üîó Related Topics

- [Vectors](../vectors/) - Basic operations and dot products
- [Matrices](../matrices/) - Matrix norms and transformations
- [Hilbert Spaces](./hilbert-spaces/) - Inner product spaces with norms
- [Banach Spaces](./banach-spaces/) - Complete normed spaces
- [Linear Operators](../linear-operators/) - Operators on normed spaces
