# Hilbert Spaces

## üìñ What You'll Learn

Hilbert spaces are complete inner product spaces where geometry works perfectly‚Äîorthogonality, projections, and angles are all well-defined. They provide the mathematical foundation for many modern machine learning techniques, especially kernel methods and infinite-dimensional learning theory.

## üìÇ Subtopics

This section contains the following specialized application:

1. **[Kernel Methods & RKHS](./kernel-methods-rkhs/)** - Reproducing Kernel Hilbert Spaces for SVMs, Gaussian Processes, and Neural Tangent Kernels

### Core Concepts

1. **Inner Product Spaces**
   - Definition: Vector space with inner product ‚ü®¬∑,¬∑‚ü© satisfying:
     - Linearity: ‚ü®Œ±x + Œ≤y, z‚ü© = Œ±‚ü®x,z‚ü© + Œ≤‚ü®y,z‚ü©
     - Symmetry: ‚ü®x,y‚ü© = ‚ü®y,x‚ü© (or conjugate symmetry for complex spaces)
     - Positive definiteness: ‚ü®x,x‚ü© ‚â• 0, with equality iff x = 0
   - Induced norm: ‚Äñx‚Äñ = ‚àö‚ü®x,x‚ü©
   - Examples: ‚Ñù‚Åø with dot product, L¬≤[a,b] with ‚à´f(x)g(x)dx

2. **Hilbert Space Definition**
   - Complete inner product space
   - Completeness: Every Cauchy sequence converges
   - Combines algebraic structure (vector space) with geometric structure (inner product) and topological structure (completeness)
   - Finite-dimensional: Every inner product space is a Hilbert space
   - Infinite-dimensional: L¬≤, ‚Ñì¬≤, function spaces

3. **Orthogonality & Orthonormal Bases**
   - Orthogonal vectors: ‚ü®x,y‚ü© = 0
   - Orthonormal basis: {e·µ¢} where ‚ü®e·µ¢,e‚±º‚ü© = Œ¥·µ¢‚±º
   - Gram-Schmidt orthogonalization
   - Expansion: x = Œ£‚ü®x,e·µ¢‚ü©e·µ¢ (generalized Fourier series)
   - Parseval's identity: ‚Äñx‚Äñ¬≤ = Œ£|‚ü®x,e·µ¢‚ü©|¬≤

4. **Projection Theorem**
   - Every x ‚àà H has unique decomposition: x = y + z
   - y ‚àà M (closed subspace), z ‚ä• M
   - y = P_M(x) is the orthogonal projection
   - Minimizes distance: ‚Äñx - y‚Äñ = min{‚Äñx - m‚Äñ : m ‚àà M}
   - Applications: least squares, best approximation

5. **Riesz Representation Theorem**
   - Every continuous linear functional f on H has form f(x) = ‚ü®x,y‚ü© for unique y ‚àà H
   - Establishes correspondence between H and its dual H*
   - Hilbert spaces are self-dual
   - Foundation for kernel methods

6. **Reproducing Kernels**
   - Kernel function: K : X √ó X ‚Üí ‚Ñù
   - Positive definiteness: Œ£·µ¢‚±ºc·µ¢c‚±ºK(x·µ¢,x‚±º) ‚â• 0
   - Reproducing property: f(x) = ‚ü®f, K(x,¬∑)‚ü©_H
   - Moore-Aronszajn theorem: K ‚Üî unique RKHS

7. **Operators on Hilbert Spaces**
   - Bounded linear operators: ‚ÄñT‚Äñ = sup{‚ÄñTx‚Äñ : ‚Äñx‚Äñ ‚â§ 1}
   - Adjoint operator: ‚ü®Tx,y‚ü© = ‚ü®x,T*y‚ü©
   - Self-adjoint (Hermitian): T = T*
   - Compact operators: map bounded sets to relatively compact sets
   - Spectral theorem for compact self-adjoint operators

## ü§ñ Machine Learning Applications

### Reproducing Kernel Hilbert Space (RKHS)
- **Kernel Trick**: Implicitly map to high/infinite-dimensional space
- **Support Vector Machines (SVMs)**:
  - Optimization in RKHS
  - Maximum margin classification
  - Kernel functions: linear, polynomial, RBF/Gaussian
  - Decision function: f(x) = Œ£Œ±·µ¢y·µ¢K(x,x·µ¢) + b
  
- **Gaussian Processes**:
  - Prior over functions in RKHS
  - Kernel defines covariance structure
  - Bayesian inference for regression/classification
  - Uncertainty quantification

### Neural Tangent Kernel (NTK)
- **Infinite-Width Neural Networks**:
  - Limit as width ‚Üí ‚àû yields kernel method
  - Training dynamics become linear in function space
  - Deterministic kernel characterizing network architecture
  - Connection between deep learning and kernel methods
  
- **Theoretical Deep Learning**:
  - Convergence guarantees for gradient descent
  - Generalization bounds via RKHS theory
  - Understanding neural network function spaces
  - Lazy training regime analysis

### Kernel Methods
- **Kernel PCA**: Nonlinear dimensionality reduction in RKHS
- **Kernel Ridge Regression**: Regularized regression with kernels
- **Kernel k-Means**: Clustering in feature space
- **Spectral Clustering**: Graph kernels and eigenvectors
- **Multiple Kernel Learning**: Combining multiple kernels

### Function Approximation
- **Best Approximation**: Projection onto finite-dimensional subspaces
- **Universal Approximation**: Dense subspaces in function spaces
- **Spline Interpolation**: Minimizing smoothness functionals
- **Radial Basis Functions**: Kernel-based interpolation

### Representation Learning
- **Feature Maps**: œÜ : X ‚Üí H where K(x,y) = ‚ü®œÜ(x),œÜ(y)‚ü©
- **Implicit Feature Spaces**: Never compute œÜ explicitly
- **Kernel Embeddings**: Distributions in RKHS
- **Maximum Mean Discrepancy (MMD)**: Distance between distributions

### Optimization in Hilbert Spaces
- **Variational Methods**: Minimizing functionals
- **Gradient Descent in Function Space**: Infinite-dimensional optimization
- **Regularization**: Penalties on RKHS norm
- **Representer Theorem**: Optimal solutions in span of kernel functions

## üìä Topics Covered

### Theoretical Foundations
- Inner product axioms and properties
- Cauchy-Schwarz and triangle inequalities
- Completeness and convergence
- Orthonormal bases and expansions
- Projection theorem and applications
- Riesz representation theorem
- Duality of Hilbert spaces

### Kernel Theory
- Positive definite kernels
- Mercer's theorem
- Moore-Aronszajn theorem
- Reproducing property
- Common kernels: RBF, polynomial, Mat√©rn
- Kernel composition and construction
- Kernel embeddings

### Computational Methods
- Gram-Schmidt orthogonalization
- QR decomposition
- Computing projections
- Kernel matrix computation
- Eigendecomposition of kernel matrices
- Nystr√∂m approximation
- Random Fourier features

### Advanced Concepts
- Spectral theorem for compact operators
- Weak and strong convergence
- Separable Hilbert spaces
- Sobolev spaces
- Bergman spaces
- Hardy spaces

## üíª What's Included

- **Comprehensive Theory**: Inner products, completeness, and kernels
- **Visual Intuition**: Geometric interpretations of projections and orthogonality
- **Python Implementations**: NumPy, SciPy, scikit-learn code for:
  - Computing inner products and norms
  - Gram-Schmidt orthogonalization
  - Orthogonal projections
  - Kernel matrix construction
  - SVM with various kernels
  - Gaussian Process regression
  - Kernel PCA
  - Neural Tangent Kernel computation
- **Interactive Examples**: Visualizing RKHS and kernel methods
- **ML Applications**: Real datasets with kernel methods
- **Exercises**: Theory and implementation problems
- **Mathematical Derivations**: Proofs of key theorems

## üéØ Learning Outcomes

By the end of this section, you will be able to:

‚úÖ Understand inner products and Hilbert space structure  
‚úÖ Apply orthogonal projections for best approximation  
‚úÖ Work with reproducing kernels and RKHS  
‚úÖ Implement Support Vector Machines with kernel trick  
‚úÖ Apply Gaussian Processes for regression/classification  
‚úÖ Understand Neural Tangent Kernel theory  
‚úÖ Choose appropriate kernels for different problems  
‚úÖ Compute kernel matrices and solve kernel methods  
‚úÖ Apply representer theorem for optimization  
‚úÖ Connect infinite-dimensional theory to finite implementations  

## üìê Mathematical Summary

### Key Definitions

**Inner Product Properties:**
```
‚ü®x,y‚ü© = ‚ü®y,x‚ü©                    (Symmetry)
‚ü®Œ±x + Œ≤y, z‚ü© = Œ±‚ü®x,z‚ü© + Œ≤‚ü®y,z‚ü©   (Linearity)
‚ü®x,x‚ü© ‚â• 0, = 0 iff x = 0        (Positive definiteness)

Induced norm: ‚Äñx‚Äñ = ‚àö‚ü®x,x‚ü©
```

**Cauchy-Schwarz Inequality:**
```
|‚ü®x,y‚ü©| ‚â§ ‚Äñx‚Äñ¬∑‚Äñy‚Äñ

Equality iff x and y are linearly dependent
```

**Projection Theorem:**
```
For closed subspace M ‚äÇ H:
x = P_M(x) + (x - P_M(x))
where P_M(x) ‚àà M and (x - P_M(x)) ‚ä• M

P_M(x) = argmin{‚Äñx - m‚Äñ : m ‚àà M}
```

**Reproducing Property:**
```
For RKHS H with kernel K:
f(x) = ‚ü®f, K(x,¬∑)‚ü©_H  for all f ‚àà H

‚ÄñK(x,¬∑)‚Äñ¬≤_H = K(x,x)
```

**Representer Theorem:**
```
For regularized optimization:
min_f L(f) + Œª‚Äñf‚Äñ¬≤_H

Optimal solution: f*(x) = Œ£·µ¢Œ±·µ¢ K(x,x·µ¢)
```

**Common Kernels:**
```
Linear:     K(x,y) = ‚ü®x,y‚ü©
Polynomial: K(x,y) = (‚ü®x,y‚ü© + c)^d
RBF/Gaussian: K(x,y) = exp(-‚Äñx-y‚Äñ¬≤/(2œÉ¬≤))
Mat√©rn:     K(x,y) = (2^(1-ŒΩ)/Œì(ŒΩ))(‚àö(2ŒΩ)r/‚Ñì)^ŒΩ K_ŒΩ(‚àö(2ŒΩ)r/‚Ñì)
```

## üî¨ Example Problems

### Problem 1: Orthogonal Projection
Given vectors in ‚Ñù¬≥:
- Find orthogonal projection onto a subspace
- Compute residual and verify orthogonality
- Apply to least squares problem

### Problem 2: Kernel SVM
For a 2D classification dataset:
- Implement SVM with RBF kernel
- Visualize decision boundary
- Compare with linear SVM
- Analyze support vectors

### Problem 3: Gaussian Process Regression
Given noisy observations:
- Fit GP with different kernels
- Predict with uncertainty bounds
- Compare kernel choices
- Visualize posterior distributions

### Problem 4: Neural Tangent Kernel
For a simple neural network:
- Compute NTK at initialization
- Compare finite-width vs infinite-width
- Analyze training dynamics
- Verify kernel method correspondence

## üìö Prerequisites

- Inner products and dot products
- Normed spaces and metric spaces
- Linear algebra fundamentals
- Calculus and analysis
- Basic optimization theory
- Understanding of SVMs (helpful)

## üöÄ Next Steps

After mastering Hilbert spaces, explore:

1. **Banach Spaces** - Spaces without inner products
2. **Operator Theory** - Advanced study of linear operators
3. **Spectral Methods** - Eigenvalue problems and decompositions
4. **Functional Analysis** - General theory of function spaces
5. **Stochastic Processes** - Random elements in Hilbert spaces
6. **Differential Geometry** - Manifolds and tangent spaces

## üéì Why This Matters for ML/AI

Understanding Hilbert spaces is essential because:

- **Kernel methods are built on RKHS theory** - SVMs, GPs, kernel PCA
- **Provides infinite-dimensional perspective** - Functions as points in space
- **Geometric intuition extends to function spaces** - Angles, projections, distances
- **Theoretical foundation for deep learning** - NTK, function space view
- **Best approximation theory** - Why and how learning works
- **Regularization has geometric meaning** - RKHS norm as smoothness
- **Bridges finite and infinite dimensions** - From data to functions

Hilbert spaces unite geometry, analysis, and probability for ML theory!

## üìñ Recommended Reading

- *Functional Analysis* by Walter Rudin (Chapters 4, 12)
- *Learning with Kernels* by Sch√∂lkopf & Smola
- *Gaussian Processes for Machine Learning* by Rasmussen & Williams
- *An Introduction to RKHS* by Berlinet & Thomas-Agnan
- *Elements of Statistical Learning* by Hastie et al. (Chapter 5)
- *Neural Tangent Kernel* papers by Jacot et al.

## üîó Related Topics

- [Normed Spaces](../) - Foundation with norms
- [Kernel Methods & RKHS](./kernel-methods-rkhs/) - Detailed kernel theory
- [Banach Spaces](../banach-spaces/) - More general complete normed spaces
- [Linear Operators](../../linear-operators/) - Transformations on Hilbert spaces
- [Spectral Methods](../../matrices/spectral-methods/) - Eigenvalue decompositions
