# Vectors

## ðŸ“– What You'll Learn

Vectors represent data points, feature embeddings, and model parameters. They are fundamental building blocks in Machine Learning and AI. In this section, you'll master:

### Core Concepts

1. **Vector Representation**
   - Understanding vectors as ordered lists of numbers
   - Geometric interpretation in 2D, 3D, and higher dimensions
   - Representing data points, parameters, and embeddings

2. **Dot Product (Inner Product)**
   - Computing similarity between vectors
   - Mathematical definition and properties
   - Applications in measuring alignment and projection

3. **Vector Norm (Length/Magnitude)**
   - L1 norm (Manhattan distance)
   - L2 norm (Euclidean distance)
   - Infinity norm and other norms
   - Normalization techniques

4. **Projection**
   - Projecting one vector onto another
   - Understanding components and orthogonal decomposition
   - Applications in feature extraction

5. **Orthogonality**
   - Perpendicular vectors and independence
   - Orthonormal bases
   - Gram-Schmidt orthogonalization process

## ðŸ¤– Machine Learning Applications

### Word Embeddings (Word2Vec, GloVe)
- **Semantic Representation**: Words mapped to dense vectors capturing meaning
- **Word2Vec**: Skip-gram and CBOW architectures
- **GloVe**: Global vectors for word representation
- **Vector Arithmetic**: "king - man + woman â‰ˆ queen"
- **Transfer Learning**: Pre-trained embeddings for downstream tasks

### Similarity Measures (Cosine Similarity)
- **Cosine Similarity**: cos(Î¸) = (uÂ·v)/(â€–uâ€–â€–vâ€–)
- **Document Similarity**: Comparing text documents using TF-IDF vectors
- **Recommendation Systems**: Finding similar items or users based on embeddings
- **Image Recognition**: Comparing feature vectors from CNNs
- **Nearest Neighbor Search**: Finding similar data points

### Feature Scaling and Normalization
- **Min-Max Scaling**: Rescaling features to [0,1] range
- **Standardization (Z-score)**: Zero mean and unit variance
- **L2 Normalization**: Unit vector scaling â€–xâ€–â‚‚ = 1
- **Batch Normalization**: Normalizing layer inputs in neural networks
- **Why It Matters**: Ensures features contribute equally, improves convergence

### Other Key Applications
- **Feature Vectors**: Representing data samples in ML models
- **Gradient Vectors**: Optimization direction in gradient descent
- **Weight Vectors**: Parameters in linear models
- **Attention Mechanisms**: Query, key, and value vectors in transformers
- **Residual Connections**: Vector addition in ResNets

## ðŸ“Š Topics Covered

- Vector operations (addition, scalar multiplication)
- Angle between vectors
- Unit vectors and vector normalization
- Distance metrics (Euclidean, Manhattan, Cosine)
- Vector spaces and subspaces
- Linear independence
- Basis vectors

## ðŸ’» What's Included

- Theoretical foundations with clear explanations
- Mathematical proofs and derivations
- Python implementations using NumPy
- Visualization examples
- Practical exercises and problems
- Real-world ML/AI use cases

## ðŸŽ¯ Learning Outcomes

By the end of this section, you will be able to:
- Perform vector operations confidently
- Calculate and interpret similarity measures
- Apply vector concepts to ML problems
- Understand how embeddings work in neural networks
- Implement vector operations from scratch and with libraries

## ðŸ“š Prerequisites

- Basic algebra
- Python programming basics
- Understanding of coordinate systems

## ðŸš€ Next Steps

After mastering vectors, you'll be ready to explore **Matrices**, which extend these concepts to linear transformations and multi-dimensional data operations.
